{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved lane miles by NTA → /Users/mohamedhiba/Fall 2025/datathon/data/processed/lane_miles_by_nta_manhattan.csv (NTAs: 23, total miles: 35.2)\n",
            "   nta2020                                      ntaname  length_miles\n",
            "10  MN0603                         Murray Hill-Kips Bay      4.410495\n",
            "3   MN0303                                 East Village      2.960079\n",
            "14  MN0801  Upper East Side-Lenox Hill-Roosevelt Island      2.718747\n",
            "6   MN0501          Midtown South-Flatiron-Union Square      2.591855\n",
            "20  MN1102                          East Harlem (North)      2.442016\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Compute Manhattan lane miles by NTA (exposure)\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import shape\n",
        "from pathlib import Path\n",
        "\n",
        "PROCESSED = Path.cwd().parent / \"data\" / \"processed\"\n",
        "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def fetch_json(url, params):\n",
        "    r = requests.get(url, params=params, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "# Fetch bus lanes (Manhattan) with geometry intact\n",
        "lanes = fetch_json(\n",
        "    \"https://data.cityofnewyork.us/resource/ycrg-ses3.json\",\n",
        "    {\"$where\": \"upper(boro) like '%MAN%'\", \"$limit\": 50000}\n",
        ")\n",
        "lanes_df = pd.DataFrame(lanes)\n",
        "lanes_gdf = gpd.GeoDataFrame(\n",
        "    lanes_df,\n",
        "    geometry=[shape(g) if isinstance(g, dict) else None for g in lanes_df.get(\"the_geom\", [])],\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "# Keep true bus lanes if present\n",
        "if \"lane_type1\" in lanes_gdf.columns:\n",
        "    lanes_gdf = lanes_gdf[lanes_gdf[\"lane_type1\"].str.contains(\"Bus Lane\", na=False)]\n",
        "\n",
        "# Fetch NTAs (Manhattan) with geometry\n",
        "ntas = fetch_json(\n",
        "    \"https://data.cityofnewyork.us/resource/9nt8-h7nd.json\",\n",
        "    {\"$where\": \"borocode=1\", \"$limit\": 50000}\n",
        ")\n",
        "ntas_df = pd.DataFrame(ntas)\n",
        "ntas_gdf = gpd.GeoDataFrame(\n",
        "    ntas_df,\n",
        "    geometry=[shape(g) if isinstance(g, dict) else None for g in ntas_df.get(\"the_geom\", [])],\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Project to feet (NY State Plane) and compute length by NTA\n",
        "lanes_2263 = lanes_gdf.to_crs(2263)\n",
        "ntas_2263 = ntas_gdf.to_crs(2263)\n",
        "\n",
        "# Clip lines to polygons, then sum lengths per NTA\n",
        "clipped = gpd.overlay(lanes_2263, ntas_2263[[\"nta2020\", \"ntaname\", \"geometry\"]], how=\"intersection\")\n",
        "clipped[\"length_miles\"] = clipped.length / 5280.0\n",
        "lane_miles = (\n",
        "    clipped.groupby([\"nta2020\", \"ntaname\"], as_index=False)[\"length_miles\"].sum()\n",
        "    .sort_values(\"length_miles\", ascending=False)\n",
        ")\n",
        "\n",
        "out = PROCESSED / \"lane_miles_by_nta_manhattan.csv\"\n",
        "lane_miles.to_csv(out, index=False)\n",
        "print(f\"Saved lane miles by NTA → {out} (NTAs: {lane_miles.shape[0]}, total miles: {lane_miles['length_miles'].sum():.1f})\")\n",
        "print(lane_miles.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 551 NTAs: 23\n",
            "    nta2020  hour          rate\n",
            "735  MN1203    16  24589.580432\n",
            "396  MN0702    13  21602.554201\n",
            "514  MN0901    11  20786.494974\n",
            "397  MN0702    14  19809.689159\n",
            "513  MN0901    10  19743.361028\n",
            "394  MN0702    11  19310.038245\n",
            "515  MN0901    12  19145.610116\n",
            "395  MN0702    12  18146.145529\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Join ACE → NTA, aggregate by NTA × hour, compute rates\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, shape\n",
        "from pathlib import Path #just to get the path and navigate through the data \n",
        "import requests\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "RAW = DATA / \"raw\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "\n",
        "# Load lane miles (from Step 1)\n",
        "lane_miles = pd.read_csv(PROCESSED / \"lane_miles_by_nta_manhattan.csv\")\n",
        "lane_miles = lane_miles.rename(columns={\"length_miles\": \"lane_miles\"})\n",
        "\n",
        "# Load ACE violations (Manhattan slice)\n",
        "ace_path = RAW / \"ace_violations_manhattan.csv\"\n",
        "ace_df = pd.read_csv(ace_path)\n",
        "if ace_df.empty:\n",
        "    raise RuntimeError(\"ACE file is empty. Run the fetch notebook cell first.\")\n",
        "\n",
        "# Extract coordinates, build GeoDataFrame\n",
        "ace_df[\"violation_latitude\"] = pd.to_numeric(ace_df.get(\"violation_latitude\"), errors=\"coerce\")\n",
        "ace_df[\"violation_longitude\"] = pd.to_numeric(ace_df.get(\"violation_longitude\"), errors=\"coerce\")\n",
        "ace_points = ace_df.dropna(subset=[\"violation_latitude\", \"violation_longitude\"]).copy()\n",
        "ace_gdf = gpd.GeoDataFrame(\n",
        "    ace_points,\n",
        "    geometry=[Point(xy) for xy in zip(ace_points[\"violation_longitude\"], ace_points[\"violation_latitude\"])],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(2263)\n",
        "\n",
        "# Hour of day from first_occurrence\n",
        "ace_gdf[\"first_occurrence\"] = pd.to_datetime(ace_gdf[\"first_occurrence\"], errors=\"coerce\")\n",
        "ace_gdf[\"hour\"] = ace_gdf[\"first_occurrence\"].dt.hour\n",
        "\n",
        "# Fetch NTA geometry (Manhattan) for spatial join\n",
        "r = requests.get(\n",
        "    \"https://data.cityofnewyork.us/resource/9nt8-h7nd.json\",\n",
        "    params={\"$where\": \"borocode=1\", \"$limit\": 50000}, timeout=60\n",
        ")\n",
        "r.raise_for_status()\n",
        "ntas_df = pd.DataFrame(r.json())\n",
        "ntas_gdf = gpd.GeoDataFrame(\n",
        "    ntas_df,\n",
        "    geometry=[shape(g) if isinstance(g, dict) else None for g in ntas_df.get(\"the_geom\", [])],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(2263)\n",
        "\n",
        "# Spatial join points → polygons\n",
        "joined = gpd.sjoin(ace_gdf, ntas_gdf[[\"nta2020\", \"ntaname\", \"geometry\"]], how=\"inner\", predicate=\"within\")\n",
        "\n",
        "# Aggregate violations by NTA × hour\n",
        "viol_by_nta_hour = (\n",
        "    joined.groupby([\"nta2020\", \"ntaname\", \"hour\"], as_index=False).size()\n",
        "    .rename(columns={\"size\": \"violations\"})\n",
        ")\n",
        "\n",
        "# Compute rates\n",
        "rates = viol_by_nta_hour.merge(lane_miles, on=[\"nta2020\", \"ntaname\"], how=\"left\")\n",
        "rates = rates[rates[\"lane_miles\"] > 0]\n",
        "rates[\"rate\"] = rates[\"violations\"] / rates[\"lane_miles\"]\n",
        "\n",
        "# Save outputs\n",
        "viol_by_nta_hour.to_csv(PROCESSED / \"violations_by_nta_hour_manhattan.csv\", index=False)\n",
        "rates.to_csv(PROCESSED / \"rates_by_nta_hour_manhattan.csv\", index=False)\n",
        "\n",
        "print(\"Rows:\", len(rates), \"NTAs:\", rates['nta2020'].nunique())\n",
        "print(rates.sort_values(\"rate\", ascending=False).head(8)[[\"nta2020\", \"hour\", \"rate\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_schools_by_hour_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_hospitals_by_hour_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_schools_by_hour_mn.png /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_hospitals_by_hour_mn.png\n"
          ]
        }
      ],
      "source": [
        "# Step 3a: Proximity to schools/hospitals (100 m), hourly spikes\n",
        "import json, ast, math\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "RAW = DATA / \"raw\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FT_PER_M = 3.28084\n",
        "BUFFER_FT = 100 * FT_PER_M  # 100 meters in feet (EPSG:2263 units)\n",
        "\n",
        "# 1) Load ACE Manhattan and build GeoDataFrame with hour\n",
        "ace_df = pd.read_csv(RAW / \"ace_violations_manhattan.csv\")\n",
        "ace_df[\"violation_latitude\"] = pd.to_numeric(ace_df.get(\"violation_latitude\"), errors=\"coerce\")\n",
        "ace_df[\"violation_longitude\"] = pd.to_numeric(ace_df.get(\"violation_longitude\"), errors=\"coerce\")\n",
        "ace_points = ace_df.dropna(subset=[\"violation_latitude\", \"violation_longitude\"]).copy()\n",
        "ace_gdf = gpd.GeoDataFrame(\n",
        "    ace_points,\n",
        "    geometry=[Point(xy) for xy in zip(ace_points[\"violation_longitude\"], ace_points[\"violation_latitude\"])],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(2263)\n",
        "ace_gdf[\"first_occurrence\"] = pd.to_datetime(ace_gdf[\"first_occurrence\"], errors=\"coerce\")\n",
        "ace_gdf[\"hour\"] = ace_gdf[\"first_occurrence\"].dt.hour.fillna(0).astype(int)\n",
        "\n",
        "# 2) Load schools (latitude/longitude + school_name)\n",
        "schools_path = RAW / \"schools_manhattan.csv\"\n",
        "schools_df = pd.read_csv(schools_path)\n",
        "lat_col = next((c for c in [\"latitude\",\"LATITUDE\",\"lat\",\"Lat\"] if c in schools_df.columns), None)\n",
        "lon_col = next((c for c in [\"longitude\",\"LONGITUDE\",\"lon\",\"Lon\"] if c in schools_df.columns), None)\n",
        "name_col = \"school_name\" if \"school_name\" in schools_df.columns else None\n",
        "if not lat_col or not lon_col:\n",
        "    raise RuntimeError(\"schools_manhattan.csv is missing latitude/longitude columns.\")\n",
        "schools_gdf = gpd.GeoDataFrame(\n",
        "    schools_df.dropna(subset=[lat_col, lon_col]).copy(),\n",
        "    geometry=gpd.points_from_xy(schools_df[lon_col], schools_df[lat_col], crs=\"EPSG:4326\")\n",
        ").to_crs(2263)\n",
        "schools_buf = schools_gdf.buffer(BUFFER_FT)\n",
        "schools_buf_gdf = gpd.GeoDataFrame(schools_gdf[[name_col]] if name_col else schools_gdf[[lat_col,lon_col]], geometry=schools_buf, crs=2263)\n",
        "\n",
        "# 3) Load hospitals (location_1 point + facility_name)\n",
        "hosp_path = RAW / \"hospitals_manhattan.csv\"\n",
        "hosp_df = pd.read_csv(hosp_path)\n",
        "# Parse Socrata 'location_1' column into lon/lat\n",
        "lon_list, lat_list = [], []\n",
        "for v in hosp_df.get(\"location_1\", []):\n",
        "    if isinstance(v, str):\n",
        "        try:\n",
        "            d = ast.literal_eval(v)\n",
        "        except Exception:\n",
        "            d = None\n",
        "    elif isinstance(v, dict):\n",
        "        d = v\n",
        "    else:\n",
        "        d = None\n",
        "    if isinstance(d, dict) and isinstance(d.get(\"coordinates\"), (list, tuple)) and len(d[\"coordinates\"])==2:\n",
        "        lon_list.append(d[\"coordinates\"][0]); lat_list.append(d[\"coordinates\"][1])\n",
        "    else:\n",
        "        lon_list.append(math.nan); lat_list.append(math.nan)\n",
        "\n",
        "hosp_df[\"_lon\"] = pd.to_numeric(lon_list, errors=\"coerce\")\n",
        "hosp_df[\"_lat\"] = pd.to_numeric(lat_list, errors=\"coerce\")\n",
        "name_hosp_col = \"facility_name\" if \"facility_name\" in hosp_df.columns else None\n",
        "hosp_gdf = gpd.GeoDataFrame(\n",
        "    hosp_df.dropna(subset=[\"_lat\",\"_lon\"]).copy(),\n",
        "    geometry=gpd.points_from_xy(hosp_df[\"_lon\"], hosp_df[\"_lat\"], crs=\"EPSG:4326\")\n",
        ").to_crs(2263)\n",
        "hosp_buf = hosp_gdf.buffer(BUFFER_FT)\n",
        "hosp_buf_gdf = gpd.GeoDataFrame(hosp_gdf[[name_hosp_col]] if name_hosp_col else hosp_gdf[[\"_lat\",\"_lon\"]], geometry=hosp_buf, crs=2263)\n",
        "\n",
        "# 4) Flag proximity via spatial join (within)\n",
        "ace_with_school = gpd.sjoin(ace_gdf, schools_buf_gdf, how=\"left\", predicate=\"within\")\n",
        "ace_with_school[\"near_school\"] = ace_with_school.index_right.notna()\n",
        "ace_with_hosp = gpd.sjoin(ace_gdf, hosp_buf_gdf, how=\"left\", predicate=\"within\")\n",
        "ace_with_hosp[\"near_hospital\"] = ace_with_hosp.index_right.notna()\n",
        "\n",
        "# Merge flags into one table aligned on ACE index\n",
        "prox = ace_gdf[[\"hour\"]].copy()\n",
        "prox = prox.join(ace_with_school[\"near_school\"], how=\"left\")\n",
        "prox = prox.join(ace_with_hosp[\"near_hospital\"], how=\"left\", rsuffix=\"_h\")\n",
        "prox[\"near_school\"] = prox[\"near_school\"].fillna(False)\n",
        "prox[\"near_hospital\"] = prox[\"near_hospital\"].fillna(False)\n",
        "\n",
        "# 5) Aggregate by hour into windows\n",
        "school_windows = {\n",
        "    \"school_7_9\": [7,8,9],\n",
        "    \"school_14_16\": [14,15,16]\n",
        "}\n",
        "hosp_windows = {\n",
        "    \"hosp_6_8\": [6,7,8],\n",
        "    \"hosp_14_16\": [14,15,16],\n",
        "    \"hosp_22_23\": [22,23]\n",
        "}\n",
        "\n",
        "# schools\n",
        "sch_hour = prox.groupby([\"hour\",\"near_school\"], as_index=False).size().rename(columns={\"size\":\"count\"})\n",
        "sch_hour.to_csv(PROCESSED / \"proximity_schools_by_hour_mn.csv\", index=False)\n",
        "\n",
        "# hospitals\n",
        "hosp_hour = prox.groupby([\"hour\",\"near_hospital\"], as_index=False).size().rename(columns={\"size\":\"count\"})\n",
        "hosp_hour.to_csv(PROCESSED / \"proximity_hospitals_by_hour_mn.csv\", index=False)\n",
        "\n",
        "# 6) Quick bar charts\n",
        "plt.figure(figsize=(9,4))\n",
        "plot_df = sch_hour.pivot(index=\"hour\", columns=\"near_school\", values=\"count\").fillna(0)\n",
        "plot_df.rename(columns={True:\"Near schools\", False:\"Other\"}, inplace=True)\n",
        "plot_df.plot(kind=\"bar\", ax=plt.gca(), color=[\"#1f77b4\",\"#999999\"])\n",
        "plt.title(\"ACE violations by hour — near schools\")\n",
        "plt.xlabel(\"Hour of day\"); plt.ylabel(\"Count\")\n",
        "plt.tight_layout(); plt.savefig(PROCESSED / \"proximity_schools_by_hour_mn.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plot_df = hosp_hour.pivot(index=\"hour\", columns=\"near_hospital\", values=\"count\").fillna(0)\n",
        "plot_df.rename(columns={True:\"Near hospitals\", False:\"Other\"}, inplace=True)\n",
        "plot_df.plot(kind=\"bar\", ax=plt.gca(), color=[\"#d62728\",\"#999999\"])\n",
        "plt.title(\"ACE violations by hour — near hospitals\")\n",
        "plt.xlabel(\"Hour of day\"); plt.ylabel(\"Count\")\n",
        "plt.tight_layout(); plt.savefig(PROCESSED / \"proximity_hospitals_by_hour_mn.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "print(\"Saved:\",\n",
        "      PROCESSED / \"proximity_schools_by_hour_mn.csv\",\n",
        "      PROCESSED / \"proximity_hospitals_by_hour_mn.csv\",\n",
        "      PROCESSED / \"proximity_schools_by_hour_mn.png\",\n",
        "      PROCESSED / \"proximity_hospitals_by_hour_mn.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_schools_hourly_share_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_hospitals_hourly_share_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_schools_top_hours_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_hospitals_top_hours_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_schools_hourly_share_mn.png /Users/mohamedhiba/Fall 2025/datathon/data/processed/proximity_hospitals_hourly_share_mn.png\n"
          ]
        }
      ],
      "source": [
        "# Step 3b: Hourly shares near schools/hospitals and top-hour tables\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "\n",
        "# Load the hourly counts (fall back to files to avoid state issues)\n",
        "sch_hour = pd.read_csv(PROCESSED / \"proximity_schools_by_hour_mn.csv\")\n",
        "hosp_hour = pd.read_csv(PROCESSED / \"proximity_hospitals_by_hour_mn.csv\")\n",
        "\n",
        "# Compute shares per hour\n",
        "sch_tot = sch_hour.groupby(\"hour\", as_index=False)[\"count\"].sum().rename(columns={\"count\":\"total\"})\n",
        "sch_true = sch_hour[sch_hour[\"near_school\"]==True][[\"hour\",\"count\"]].rename(columns={\"count\":\"near\"})\n",
        "sch_share = sch_tot.merge(sch_true, on=\"hour\", how=\"left\").fillna({\"near\":0.0})\n",
        "sch_share[\"share_near_school\"] = sch_share[\"near\"] / sch_share[\"total\"].replace({0: pd.NA})\n",
        "\n",
        "hosp_tot = hosp_hour.groupby(\"hour\", as_index=False)[\"count\"].sum().rename(columns={\"count\":\"total\"})\n",
        "hosp_true = hosp_hour[hosp_hour[\"near_hospital\"]==True][[\"hour\",\"count\"]].rename(columns={\"count\":\"near\"})\n",
        "hosp_share = hosp_tot.merge(hosp_true, on=\"hour\", how=\"left\").fillna({\"near\":0.0})\n",
        "hosp_share[\"share_near_hospital\"] = hosp_share[\"near\"] / hosp_share[\"total\"].replace({0: pd.NA})\n",
        "\n",
        "# Save CSVs\n",
        "sch_share.to_csv(PROCESSED / \"proximity_schools_hourly_share_mn.csv\", index=False) \n",
        "hosp_share.to_csv(PROCESSED / \"proximity_hospitals_hourly_share_mn.csv\", index=False)\n",
        "\n",
        "# Top hours tables\n",
        "sch_top = sch_share.sort_values(\"share_near_school\", ascending=False).head(5)\n",
        "hosp_top = hosp_share.sort_values(\"share_near_hospital\", ascending=False).head(5)\n",
        "sch_top.to_csv(PROCESSED / \"proximity_schools_top_hours_mn.csv\", index=False)\n",
        "hosp_top.to_csv(PROCESSED / \"proximity_hospitals_top_hours_mn.csv\", index=False)\n",
        "\n",
        "# Plots\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.plot(sch_share[\"hour\"], sch_share[\"share_near_school\"], marker=\"o\", color=\"#1f77b4\")\n",
        "plt.title(\"Share of ACE violations near schools by hour (Manhattan)\")\n",
        "plt.xlabel(\"Hour of day\"); plt.ylabel(\"Share near schools\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout(); plt.savefig(PROCESSED / \"proximity_schools_hourly_share_mn.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.plot(hosp_share[\"hour\"], hosp_share[\"share_near_hospital\"], marker=\"o\", color=\"#d62728\")\n",
        "plt.title(\"Share of ACE violations near hospitals by hour (Manhattan)\")\n",
        "plt.xlabel(\"Hour of day\"); plt.ylabel(\"Share near hospitals\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout(); plt.savefig(PROCESSED / \"proximity_hospitals_hourly_share_mn.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "print(\"Saved:\",\n",
        "      PROCESSED / \"proximity_schools_hourly_share_mn.csv\",\n",
        "      PROCESSED / \"proximity_hospitals_hourly_share_mn.csv\",\n",
        "      PROCESSED / \"proximity_schools_top_hours_mn.csv\",\n",
        "      PROCESSED / \"proximity_hospitals_top_hours_mn.csv\",\n",
        "      PROCESSED / \"proximity_schools_hourly_share_mn.png\",\n",
        "      PROCESSED / \"proximity_hospitals_hourly_share_mn.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/mohamedhiba/Fall 2025/datathon/data/processed/rates_by_cdta_hour_manhattan.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/rates_by_cdta_overall_manhattan.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/sw/nd422j2x1hd_b_t5p52w2_640000gn/T/ipykernel_27682/4074089522.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n",
            "/var/folders/sw/nd422j2x1hd_b_t5p52w2_640000gn/T/ipykernel_27682/4074089522.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Equity prep — aggregate rates to CDTA via NTA→CDTA crosswalk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "RAW = DATA / \"raw\"\n",
        "\n",
        "# Load rates by NTA × hour (has lane_miles)\n",
        "rates_nta_hr = pd.read_csv(PROCESSED / \"rates_by_nta_hour_manhattan.csv\")\n",
        "assert {\"nta2020\",\"hour\",\"rate\",\"lane_miles\"}.issubset(rates_nta_hr.columns)\n",
        "\n",
        "# Fetch tract→NTA/CDTA crosswalk and build NTA→CDTA mapping (majority by tract count)\n",
        "r = requests.get(\n",
        "    \"https://data.cityofnewyork.us/resource/hm78-6dwm.json\",\n",
        "    params={\"$where\": \"borocode=1\", \"$select\": \"ntacode,cdtacode,count(geoid) as n\", \"$group\": \"ntacode,cdtacode\"},\n",
        "    timeout=60\n",
        ")\n",
        "r.raise_for_status()\n",
        "xwalk = pd.DataFrame(r.json())\n",
        "# Ensure proper types\n",
        "xwalk[\"n\"] = pd.to_numeric(xwalk[\"n\"], errors=\"coerce\")\n",
        "# Choose CDTA with max tract count per NTA\n",
        "nta_to_cdta = (\n",
        "    xwalk.sort_values([\"ntacode\",\"n\"], ascending=[True, False])\n",
        "         .drop_duplicates(subset=[\"ntacode\"])[[\"ntacode\",\"cdtacode\"]]\n",
        "         .rename(columns={\"ntacode\":\"nta2020\"})\n",
        ")\n",
        "\n",
        "# Merge mapping\n",
        "rates_nta_hr_map = rates_nta_hr.merge(nta_to_cdta, on=\"nta2020\", how=\"left\")\n",
        "\n",
        "# Weighted aggregation to CDTA × hour (weights = lane_miles)\n",
        "def weighted_mean(df, val=\"rate\", w=\"lane_miles\"):\n",
        "    d = df[[val, w]].dropna()\n",
        "    denom = d[w].sum()\n",
        "    return (d[val]*d[w]).sum()/denom if denom else float(\"nan\")\n",
        "\n",
        "agg = (\n",
        "    rates_nta_hr_map.groupby([\"cdtacode\",\"hour\"], as_index=False)\n",
        "    .apply(lambda g: pd.Series({\n",
        "        \"lane_miles\": g[\"lane_miles\"].sum(),\n",
        "        \"rate_wm\": weighted_mean(g, \"rate\", \"lane_miles\")\n",
        "    }))\n",
        ")\n",
        "\n",
        "# Also compute CDTA overall mean rate (weighted across hours by lane miles)\n",
        "cdta_overall = (\n",
        "    agg.groupby(\"cdtacode\", as_index=False)\n",
        "       .apply(lambda g: pd.Series({\n",
        "           \"lane_miles\": g[\"lane_miles\"].mean(),\n",
        "           \"rate_wm_overall\": g[\"rate_wm\"].mean()\n",
        "       }))\n",
        ")\n",
        "\n",
        "# Save\n",
        "agg.to_csv(PROCESSED / \"rates_by_cdta_hour_manhattan.csv\", index=False)\n",
        "cdta_overall.to_csv(PROCESSED / \"rates_by_cdta_overall_manhattan.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      PROCESSED / \"rates_by_cdta_hour_manhattan.csv\",\n",
        "      PROCESSED / \"rates_by_cdta_overall_manhattan.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved CDTA income to /Users/mohamedhiba/Fall 2025/datathon/data/raw/ccc_income_cdta.csv (rows: 12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/sw/nd422j2x1hd_b_t5p52w2_640000gn/T/ipykernel_27682/2451136448.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n"
          ]
        }
      ],
      "source": [
        "# Step 5a (auto): Fetch ACS tract income and aggregate to CDTA (creates ccc_income_cdta.csv)\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "RAW = DATA / \"raw\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "\n",
        "# ACS variables: B19013_001E (Median household income), B11001_001E (Total households)\n",
        "YEAR = 2022\n",
        "BASE = f\"https://api.census.gov/data/{YEAR}/acs/acs5\"\n",
        "VARS = [\"NAME\",\"B19013_001E\",\"B11001_001E\"]\n",
        "NY_COUNTIES = {\"005\":\"Bronx\",\"047\":\"Kings\",\"061\":\"New York\",\"081\":\"Queens\",\"085\":\"Richmond\"}\n",
        "\n",
        "rows = []\n",
        "for fips in NY_COUNTIES.keys():\n",
        "    params = {\n",
        "        \"get\": \",\".join(VARS),\n",
        "        \"for\": \"tract:*\",\n",
        "        \"in\": f\"state:36 county:{fips}\"\n",
        "    }\n",
        "    r = requests.get(BASE, params=params, timeout=60); r.raise_for_status()\n",
        "    data = r.json()\n",
        "    cols = data[0]\n",
        "    for d in data[1:]:\n",
        "        rec = dict(zip(cols, d))\n",
        "        rows.append(rec)\n",
        "acs = pd.DataFrame(rows)\n",
        "\n",
        "# Build GEOID\n",
        "acs[\"state\"] = acs[\"state\"].astype(str).str.zfill(2)\n",
        "acs[\"county\"] = acs[\"county\"].astype(str).str.zfill(3)\n",
        "acs[\"tract\"] = acs[\"tract\"].astype(str).str.zfill(6)\n",
        "acs[\"geoid\"] = acs[\"state\"] + acs[\"county\"] + acs[\"tract\"]\n",
        "acs[\"income_median\"] = pd.to_numeric(acs[\"B19013_001E\"], errors=\"coerce\")\n",
        "acs[\"households\"] = pd.to_numeric(acs[\"B11001_001E\"], errors=\"coerce\")\n",
        "acs = acs[[\"geoid\",\"income_median\",\"households\"]]\n",
        "\n",
        "# Map tracts → CDTA via hm78-6dwm (Manhattan only)\n",
        "x = requests.get(\n",
        "    \"https://data.cityofnewyork.us/resource/hm78-6dwm.json\",\n",
        "    params={\"$select\":\"geoid,cdtacode\",\"$where\":\"geoid is not null AND cdtacode like 'MN%'\"}, timeout=60\n",
        ")\n",
        "x.raise_for_status()\n",
        "xwalk = pd.DataFrame(x.json())\n",
        "tract_to_cdta = acs.merge(xwalk, on=\"geoid\", how=\"inner\")\n",
        "\n",
        "# Clean income values: drop implausible or sentinel negatives, and zero/neg households\n",
        "tract_to_cdta = tract_to_cdta[(tract_to_cdta[\"households\"]>0)]\n",
        "tract_to_cdta.loc[~tract_to_cdta[\"income_median\"].between(10000, 300000), \"income_median\"] = pd.NA\n",
        "\n",
        "# Aggregate to CDTA (household-weighted average of tract median incomes)\n",
        "def weighted_mean(group):\n",
        "    d = group[[\"income_median\",\"households\"]].dropna()\n",
        "    if d.empty:\n",
        "        return None\n",
        "    w = d[\"households\"]\n",
        "    v = d[\"income_median\"]\n",
        "    denom = w.sum()\n",
        "    return (v*w).sum()/denom if denom else None\n",
        "\n",
        "cdta_income = (tract_to_cdta.groupby(\"cdtacode\", as_index=False, group_keys=False)\n",
        "               .apply(lambda g: pd.Series({\n",
        "                   \"median_household_income\": weighted_mean(g)\n",
        "               })))\n",
        "\n",
        "out = RAW / \"ccc_income_cdta.csv\"\n",
        "cdta_income.to_csv(out, index=False)\n",
        "print(f\"Saved CDTA income to {out} (rows: {len(cdta_income)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/mohamedhiba/Fall 2025/datathon/data/processed/equity_rates_by_income_decile_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/equity_top_cdta_by_rate_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/equity_bottom_cdta_by_rate_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/equity_rate_vs_income_decile_mn.png\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Equity — join CDTA rates to income and compute deciles\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "RAW = DATA / \"raw\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "\n",
        "rates_cdta = pd.read_csv(PROCESSED / \"rates_by_cdta_overall_manhattan.csv\")\n",
        "rates_cdta[\"cdtacode\"] = rates_cdta[\"cdtacode\"].astype(str).str.upper()\n",
        "\n",
        "income_path = RAW / \"ccc_income_cdta.csv\"\n",
        "if not income_path.exists():\n",
        "    # Create a template listing all CDTAs present in rates, to be filled once\n",
        "    all_cdta = sorted(rates_cdta[\"cdtacode\"].dropna().unique())\n",
        "    tpl = pd.DataFrame({\n",
        "        \"cdtacode\": all_cdta,\n",
        "        \"median_household_income\": [np.nan]*len(all_cdta)\n",
        "    })\n",
        "    tpl.to_csv(income_path, index=False)\n",
        "    raise FileNotFoundError(f\"Missing {income_path}. A template with {len(all_cdta)} CDTAs was created; please fill median_household_income and rerun.\")\n",
        "\n",
        "income = pd.read_csv(income_path)\n",
        "income = income.rename(columns={c: c.lower() for c in income.columns})\n",
        "income[\"cdtacode\"] = income[\"cdtacode\"].astype(str).str.upper().str.strip()\n",
        "income[\"median_household_income\"] = pd.to_numeric(income[\"median_household_income\"], errors=\"coerce\")\n",
        "\n",
        "# Merge\n",
        "merged = rates_cdta.merge(income[[\"cdtacode\",\"median_household_income\"]], on=\"cdtacode\", how=\"left\")\n",
        "\n",
        "# Drop rows without income\n",
        "merged = merged.dropna(subset=[\"median_household_income\"]).copy()\n",
        "\n",
        "# Income deciles\n",
        "merged[\"income_decile\"] = pd.qcut(merged[\"median_household_income\"], q=10, labels=False, duplicates=\"drop\")\n",
        "\n",
        "# Aggregate trend\n",
        "trend = (merged.groupby(\"income_decile\", as_index=False)[\"rate_wm_overall\"].mean()\n",
        "                  .rename(columns={\"rate_wm_overall\":\"mean_rate\"}))\n",
        "trend.to_csv(PROCESSED / \"equity_rates_by_income_decile_mn.csv\", index=False)\n",
        "\n",
        "# Also save top/bottom lists\n",
        "top_cdta = merged.sort_values(\"rate_wm_overall\", ascending=False)[[\"cdtacode\",\"median_household_income\",\"rate_wm_overall\"]].head(10)\n",
        "bot_cdta = merged.sort_values(\"rate_wm_overall\", ascending=True)[[\"cdtacode\",\"median_household_income\",\"rate_wm_overall\"]].head(10)\n",
        "top_cdta.to_csv(PROCESSED / \"equity_top_cdta_by_rate_mn.csv\", index=False)\n",
        "bot_cdta.to_csv(PROCESSED / \"equity_bottom_cdta_by_rate_mn.csv\", index=False)\n",
        "\n",
        "# Plot trend\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(trend[\"income_decile\"], trend[\"mean_rate\"], marker=\"o\", color=\"#444\")\n",
        "plt.title(\"Violation rate vs income decile (CDTA, Manhattan)\")\n",
        "plt.xlabel(\"Income decile (0=lowest)\"); plt.ylabel(\"Mean rate (violations per lane-mile)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout(); plt.savefig(PROCESSED / \"equity_rate_vs_income_decile_mn.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "print(\"Saved:\", PROCESSED / \"equity_rates_by_income_decile_mn.csv\",\n",
        "      PROCESSED / \"equity_top_cdta_by_rate_mn.csv\",\n",
        "      PROCESSED / \"equity_bottom_cdta_by_rate_mn.csv\",\n",
        "      PROCESSED / \"equity_rate_vs_income_decile_mn.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/mohamedhiba/Fall 2025/datathon/data/processed/where_when_top_overall_mn.csv /Users/mohamedhiba/Fall 2025/datathon/data/processed/where_when_top_per_hour_mn.csv\n"
          ]
        }
      ],
      "source": [
        "# Step 6: \"Where and when\" — top NTA × hour by normalized rate\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "\n",
        "rates = pd.read_csv(PROCESSED / \"rates_by_nta_hour_manhattan.csv\")\n",
        "# Basic guardrails\n",
        "rates = rates[(rates[\"lane_miles\"] > 0.05)].copy()  # drop tiny denominators\n",
        "rates = rates.dropna(subset=[\"rate\"]) \n",
        "\n",
        "# Top overall windows\n",
        "top_overall = (rates.sort_values(\"rate\", ascending=False)\n",
        "                    .head(20)[[\"nta2020\",\"ntaname\",\"hour\",\"violations\",\"lane_miles\",\"rate\"]])\n",
        "top_overall.to_csv(PROCESSED / \"where_when_top_overall_mn.csv\", index=False)\n",
        "\n",
        "# Top 5 per hour\n",
        "tops = []\n",
        "for h, grp in rates.groupby(\"hour\"):\n",
        "    g = grp.sort_values(\"rate\", ascending=False).head(5)\n",
        "    g = g[[\"nta2020\",\"ntaname\",\"hour\",\"violations\",\"lane_miles\",\"rate\"]]\n",
        "    tops.append(g)\n",
        "per_hour_top = pd.concat(tops, ignore_index=True)\n",
        "per_hour_top.to_csv(PROCESSED / \"where_when_top_per_hour_mn.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\", PROCESSED / \"where_when_top_overall_mn.csv\",\n",
        "      PROCESSED / \"where_when_top_per_hour_mn.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/mohamedhiba/Fall 2025/datathon/data/processed/where_when_action_playbook_mn.csv\n"
          ]
        }
      ],
      "source": [
        "# Step 6b: Turn \"where/when\" into a short action playbook\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = Path.cwd().parent / \"data\"\n",
        "PROCESSED = DATA / \"processed\"\n",
        "\n",
        "# Load top overall NTA×hour by rate\n",
        "df = pd.read_csv(PROCESSED / \"where_when_top_overall_mn.csv\")\n",
        "\n",
        "# Tag time windows\n",
        "def window_tag(h):\n",
        "    h = int(h)\n",
        "    if h in (7,8,9):\n",
        "        return \"school_AM (7–9)\"\n",
        "    if h in (14,15,16):\n",
        "        return \"school_PM (14–16)\"\n",
        "    if h in (6,7,8):\n",
        "        return \"hospital_AM (6–8)\"\n",
        "    if h in (22,23):\n",
        "        return \"hospital_late (22–23)\"\n",
        "    return \"other\"\n",
        "\n",
        "df[\"window\"] = df[\"hour\"].apply(window_tag)\n",
        "\n",
        "# Simple recommendation text\n",
        "def recommend(row):\n",
        "    w = row[\"window\"]\n",
        "    if w.startswith(\"school_AM\") or w.startswith(\"school_PM\"):\n",
        "        return \"Post-stop signed loading zone for drop-off/pick-up; targeted enforcement in window\"\n",
        "    if w.startswith(\"hospital\"):\n",
        "        return \"Time-based loading curb and brief tow window aligned to shift times\"\n",
        "    return \"Light protection or signage; monitor and deploy targeted enforcement\"\n",
        "\n",
        "df[\"recommendation\"] = df.apply(recommend, axis=1)\n",
        "\n",
        "# Clean columns and round\n",
        "out = df[[\"nta2020\",\"ntaname\",\"hour\",\"lane_miles\",\"violations\",\"rate\",\"window\",\"recommendation\"]].copy()\n",
        "out[\"rate\"] = out[\"rate\"].round(2)\n",
        "\n",
        "# Keep top 12 for a 1-pager\n",
        "playbook = out.head(12)\n",
        "playbook.to_csv(PROCESSED / \"where_when_action_playbook_mn.csv\", index=False)\n",
        "print(\"Saved:\", PROCESSED / \"where_when_action_playbook_mn.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
